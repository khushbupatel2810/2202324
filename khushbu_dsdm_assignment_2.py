# -*- coding: utf-8 -*-
"""KHUSHBU DSDM ASSIGNMENT 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fbjmiSw0t2eXW4yxaMwkJ2BXf4vRFfyg
"""

"""DSDM2.ipynb


Name: Khushbu Patel (kp22472)
Reg. No.:
Supervisor: Dr Ana Matran-Fernandez
"""

import pandas as pn
import numpy as np
import matplotlib.pyplot as pyplot
import os
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import KNNImputer,IterativeImputer,SimpleImputer
import seaborn as sn

# the process of loading the .tsv file for a particular participant from the collection of 60 files in the dataset to carry out our tasks.
# Certain files are quite large and are leading to memory-related problems. To alleviate the strain on the machine, we can opt to load a small percentage of the data from those files.

# num_rows = int(len(pn.read_csv('Participant0010.tsv', sep='\t')) * 0.1)
# df = pn.read_csv('Participant0010.tsv', sep='\t', nrows=num_rows)

df = pn.read_csv('Participant0001.tsv', sep='\t')

"""** Reading Data**"""

df.shape
df.head(10)
df.tail(10)

# Examine an overview of statistical information for the dataset.
df.describe()

df.columns

# bar plot for project name
pyplot.figure(figsize=(10,6))
sn.countplot(y='Project name', data=df)
pyplot.title('Project Name Count Plot')
pyplot.show()

# convert the 'Recording timestamp' column to datetime
df['Recording timestamp'] = pn.to_datetime(df['Recording timestamp'])

# set the 'Recording timestamp' column as the index of the DataFrame
df.set_index('Recording timestamp', inplace=True)

# plot the time series
df.plot(figsize=(10, 5))

# add labels and title to the plot
pyplot.xlabel('Time')
pyplot.ylabel('Y-axis ')
pyplot.title('recording timestamps')

# show the plot
pyplot.show()

# Bar chart depicting eye movement categories.
df['Eye movement type'].value_counts().plot.bar(figsize=(3,3))

# Generate a scatter plot illustrating the relationship between Fixation Point X and Fixation Point Y.
pyplot.scatter(df['Fixation point X'], df['Fixation point Y'])

# Assign plot title and axis captions.
pyplot.title('Fixation point X vs. Fixation point Y')
pyplot.xlabel('Fixation point X')
pyplot.ylabel('Fixation point Y')

# Present the plot.
pyplot.show()

# Generate a scatter plot illustrating the correlation between Gaze Point X and Gaze Point Y.
pyplot.scatter(df['Gaze point X'], df['Gaze point Y'])
pyplot.xlabel('Gaze point X')
pyplot.ylabel('Gaze point Y')
pyplot.title('Scatter plot of Gaze point X vs. Gaze point Y ')
pyplot.show()

"""Data Cleaning"""

df.columns

# Identify columns containing over 50% missing values.
null_percent = df.isnull().sum()/len(df) * 100
# Columns for removal.
cols_to_drop = null_percent[null_percent >= 50].index

print("Columns to drop are:")
print(cols_to_drop)

#  Eliminate/discard the columns.
df.drop(cols_to_drop, axis=1, inplace=True)

# Remove columns with a consistent single value across the dataset.
# Iterate through the columns to identify those with a uniform single value.
for col in df.columns:
    if len(df[col].unique()) == 1:
      # drop that column
        df.drop(col,inplace=True,axis=1)

# Let's observe the count of null values present.
df.isna().sum()

# Addressing missing values. We are dealing with both numerical and categorical data; discarding null values would lead to data reduction, however...
# Based on specific columns, I have opted to retain them. Instead of discarding categorical null values, I will replace them with the term "unknown."

# Impute the missing values in categorical data columns with the label "unknown."
object_cols = df.select_dtypes(include=['object']).columns
df[object_cols] = df[object_cols].fillna(df[object_cols].mode().iloc[0])

# Extract solely the numerical columns from the dataframe
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
# Replace the absent values with the mean of their corresponding numeric columns.
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

# Verifying/null value inspection at the moment
df.isna().sum()

df.hist(figsize=(30,30))

"""**Correlation Examination** """

# Choose exclusively the columns with numerical data from the dataframe
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
# Substitute missing values with the average of the corresponding numeric columns.
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Conduct multicollinearity assessment on the columns with float data type
float_cols = df.select_dtypes(exclude=['object']).columns
X = df[float_cols].dropna()
vif = pn.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif["features"] = X.columns

X.corr()

# Generate a heatmap illustrating the correlation matrix for columns with float data type
corr = X.corr()
pyplot.figure(figsize=(12,10))
sn.heatmap(corr, annot=True, cmap='coolwarm')
pyplot.title('Correlation matrix heatmap')

pyplot.show()

# Create a scatter plot
pyplot.scatter(df['Fixation point X'], df['Gaze point X'], label='Fixation point X')
pyplot.scatter(df['Gaze point right X'], df['Gaze point X'], label='Gaze point right X')
pyplot.xlabel('Features')
pyplot.ylabel('Target Variable')
pyplot.legend()
pyplot.show()

# Generate a box plot.
df.boxplot(column=['Gaze point X','Fixation point X', 'Gaze point right X'])
pyplot.show()

# We can further enhance data cleanliness by eliminating columns with significant correlation.

# Identify strongly correlated features.
highly_correlated = []
threshold = 0.7
for i in range(len(corr.columns)):
    for j in range(i):
        if abs(corr.iloc[i, j]) > threshold:
            colname = corr.columns[i]
            highly_correlated.append(colname)

print('Highly correlated columns are:')
highly_correlated

# Exclude features with high correlation
df = df.drop(columns=highly_correlated)

# Current state of the dataframe
df

"""'''**Feature Engineering**'''

"""

# Now execute one-hot encoding on this sanitized data and subsequently apply PCA once more
# I will utilize LSTM to determine which yields favorable outcomes

# Import the required/essential modules/libraries/packages
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras.models import Sequential
from keras.layers import Dense, LSTM

from sklearn.preprocessing import OneHotEncoder
from scipy.sparse import hstack

# Segment categorical and numeric attributes
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Normalize numeric data
num_scaler = MinMaxScaler()
num_data = num_scaler.fit_transform(df[numerical_cols])

# Since we possess categorical data, we must conduct one-hot encoding on it
# In this context, I am utilizing the OneHotEncoder() function from the sklearn preprocessing module
# Perform one-hot encoding on categorical data
cat_encoder = OneHotEncoder(sparse=True)
cat_data = cat_encoder.fit_transform(df[categorical_cols])

# Merge numerical and categorical data
combined_data = np.hstack((num_data, cat_data.toarray()))

combined_data

# PCA
pca = PCA(n_components=10)
pca_data = pca.fit_transform(combined_data)

# print pca_data
pca_data

"""**LSTM**"""

# Divide into training and testing sets
train_size = int(len(pca_data) * 0.8)
train_data, test_data = pca_data[0:train_size,:], pca_data[train_size:len(df),:]

# Transform data into time series format

# This function generates a time series dataset where the X values constitute a sequence of temporal steps, and the y value
# represents the subsequent time step following the sequence.
def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        # Extract a sequence of time_steps from X, commencing at index i
        # and add it to the input sequences Xs.
        Xs.append(X[i:(i+time_steps), :])
        # Retrieve the subsequent value from the target dataset y and include it in the target values ys.
        ys.append(y[i+time_steps])
    return np.array(Xs), np.array(ys)

time_steps = 10 # flexible
# Define the training and testing datasets
X_train, y_train = create_dataset(train_data, train_data, time_steps)
X_test, y_test = create_dataset(test_data, test_data, time_steps)

# Construct an LSTM model
model = Sequential()

# Incorporate an LSTM layer comprising 64 units/neurons, and configure the input shape to align with the training data's shape
model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])))

# Integrate a Dense layer featuring 64 units/neurons and utilizing the ReLU activation function
model.add(Dense(64, activation='relu'))

# Include a Dense layer with the same number of units as the count of features in the training data
model.add(Dense(X_train.shape[2]))

# Configure the model by compiling it with the Adam optimizer and employing the mean squared error loss function
model.compile(optimizer='adam', loss='mse')

# Train the model
history=model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=1)

# Generate predictions on the test set
y_pred = model.predict(X_test)

import matplotlib.pyplot as pyplot
# Create a plot illustrating the variation of training and validation loss across epochs
# Generate a graph depicting the relationship between epochs and loss.
pyplot.plot(history.history['loss'])
pyplot.title('LSTM Model Loss')
pyplot.ylabel('Loss')
pyplot.xlabel('Epoch')
pyplot.show()

# # Transform y_test and y_pred into 2-dimensional arrays
# y_test = y_test.reshape(-1, pca_data.shape[1])
# y_pred = y_pred.reshape(-1, pca_data.shape[1])

# # Revert the transformed predictions and true values to their original form
# y_pred = scaler.inverse_transform(y_pred)
# y_test = scaler.inverse_transform(y_test)

# Calculate Mean Squared Error (MSE) and Mean Absolute Error (MAE).
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Print scores
print('MSE:', mse)
print('MAE:', mae)